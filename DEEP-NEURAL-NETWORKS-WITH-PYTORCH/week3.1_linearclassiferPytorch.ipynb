{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-10-03 16:53:15--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip\n",
      "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
      "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 245259777 (234M) [application/zip]\n",
      "Saving to: ‘/resources/data/concrete_crack_images_for_classification.zip.11’\n",
      "\n",
      "concrete_crack_imag 100%[===================>] 233.90M  26.1MB/s    in 12s     \n",
      "\n",
      "2020-10-03 16:53:27 (20.1 MB/s) - ‘/resources/data/concrete_crack_images_for_classification.zip.11’ saved [245259777/245259777]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip -P /resources/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /resources/data/concrete_crack_images_for_classification.zip\n"
     ]
    }
   ],
   "source": [
    "!unzip -n  /resources/data/concrete_crack_images_for_classification.zip -d  /resources/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch import optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    \n",
    "    #constructor\n",
    "    def __init__(self, transform=None, train=True):\n",
    "        directory = \"/resources/data\"\n",
    "        positive = \"Positive\"\n",
    "        negative = \"Negative\"\n",
    "        \n",
    "        positive_file_path = os.path.join(directory, positive)\n",
    "        negative_file_path = os.path.join(directory, negative)\n",
    "        \n",
    "        positive_files = [os.path.join(positive_file_path, file) for file in os.listdir(positive_file_path) if file.endswith(\".jpg\")]\n",
    "        positive_files.sort()\n",
    "        negative_files = [os.path.join(negative_file_path, file) for file in os.listdir(negative_file_path) if file.endswith(\".jpg\")]\n",
    "        negative_files.sort()\n",
    "        \n",
    "        number_of_samples = len(positive_files) + len(negative_files)\n",
    "        self.all_files = [None] * number_of_samples\n",
    "        self.all_files[::2] = positive_files\n",
    "        self.all_files[1::2] = negative_files\n",
    "        \n",
    "        # Transform is going to be used on image\n",
    "        self.transform = transform\n",
    "        \n",
    "        # torch.LongTensor\n",
    "        self.Y = torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
    "        self.Y[::2] = 1\n",
    "        self.Y[1::2] = 0\n",
    "        \n",
    "        if train:\n",
    "            self.all_files = self.all_files[0:30000]\n",
    "            self.Y = self.Y[0:30000]\n",
    "            self.len = len(self.all_files)\n",
    "        else:\n",
    "            self.all_files = self.all_files[30000:]\n",
    "            self.Y = self.Y[30000:]\n",
    "            self.len = len(self.all_files)\n",
    "    \n",
    "    # get length\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    # getter\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.all_files[idx])\n",
    "        y = self.Y[idx]\n",
    "        \n",
    "        # If there is any transform method, apply it onto the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORM OBJECT AND DATASET OBJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset(transform = transform, train = True)\n",
    "dataset_val = Dataset(transform = transform, train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 227, 227])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0][0].shape\n",
    "#dataset_train.view(-1, 154587)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154587"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_of_image = 3 * 227 * 227\n",
    "size_of_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fadcf78bb70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class Net(nn.Module):\n",
    "    \n",
    "#    def __init__(self, in_size= size_of_image, out_size=2):\n",
    "#        super(Net,self).__init__()\n",
    "#        self.linear1 = nn.Linear(in_size, out_size)\n",
    "    \n",
    "#    def forward(self, x):\n",
    "#        #x = self.linear1(x)\n",
    "#        x = x.view(-1, 227 * 227*3)\n",
    "#        x = self.linear1(x)\n",
    "        \n",
    "#        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size = size_of_image, out_size = 2, bias = None):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_size, out_size)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3 * 227 * 227)\n",
    "        x = self.linear1(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL OBJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear1.weight',\n",
       "              tensor([[-1.9042e-05,  1.3644e-03, -2.0933e-03,  ..., -2.2041e-03,\n",
       "                       -1.6752e-03, -6.4412e-04],\n",
       "                      [ 5.9658e-04,  1.8556e-03, -9.0110e-04,  ..., -3.9324e-04,\n",
       "                       -4.3716e-04, -3.5319e-04]])),\n",
       "             ('linear1.bias', tensor([-0.0004, -0.0008]))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRITERION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATALOADER TRAINING AND VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = dataset_train, batch_size = 1000)\n",
    "val_loader = DataLoader(dataset = dataset_val, batch_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN MODEL WITH 5 EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "LOSS = []\n",
    "ACCURACY = []\n",
    "correct = 0\n",
    "n_test = len(dataset_val)\n",
    "#Softmax_fn=nn.Softmax(dim=-1)\n",
    "\n",
    "def train_model(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        loss_sub = []\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            z = model(x)\n",
    "            loss = criterion(z, y)\n",
    "            loss_sub.append(loss.data.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        LOSS.append(np.mean(loss_sub))\n",
    "        #print(\"probability of class(training) \", torch.max(Softmax_fn(z)).item())\n",
    "\n",
    "\n",
    "        \n",
    "        correct = 0\n",
    "        for x_test, y_test in val_loader:\n",
    "            z = model(x_test)\n",
    "            _, yhat = torch.max(z.data, 1)\n",
    "            correct += (yhat == y_test).sum().item()\n",
    "        accuracy = correct / n_test\n",
    "        ACCURACY.append(accuracy)\n",
    "        print('Epoch ' + str(epoch) + ':' + str(accuracy))\n",
    "        #print(\"probability of class(testing) \", torch.max(Softmax_fn(z)).item())\n",
    "\n",
    "\n",
    "    \n",
    "train_model(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import ndimage, misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create 2-dimentional convolution object\n",
    "conv = nn.Conv2d(in_channels = 1, out_channels = 1, kernel_size = 3)\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[[[ 1.,  0., -1.],\n",
       "                        [ 2.,  0., -2.],\n",
       "                        [ 1.,  0., -1.]]]])),\n",
       "             ('bias', tensor([0.]))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.state_dict()['weight'][0][0]=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])\n",
    "conv.state_dict()['bias'][0] = 0.0\n",
    "conv.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 1., 0., 0.],\n",
       "          [0., 0., 1., 0., 0.],\n",
       "          [0., 0., 1., 0., 0.],\n",
       "          [0., 0., 1., 0., 0.],\n",
       "          [0., 0., 1., 0., 0.]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.zeros(1, 1, 5, 5)\n",
    "# number of image in our tensor, grayscale image, 5x5 pixels image\n",
    "image[0,0,:,2] = 1\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-4.,  0.,  4.],\n",
       "          [-4.,  0.,  4.],\n",
       "          [-4.,  0.,  4.]]]], grad_fn=<MkldnnConvolutionBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = conv(image)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[[[1., 1.],\n",
       "                        [1., 1.]]]])),\n",
       "             ('bias', tensor([0.]))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determining the size of the output\n",
    "\n",
    "K = 2\n",
    "# Kernel size=2\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=K)\n",
    "conv1.state_dict()['weight'][0][0] = torch.tensor([[1.0, 1.0],\n",
    "                                                   [1.0, 1.0]\n",
    "    \n",
    "])\n",
    "conv1.state_dict()['bias'][0] = 0.0\n",
    "conv1.state_dict()\n",
    "#conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 4\n",
    "image1 = torch.ones(1, 1, M, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z1: tensor([[[[4., 4., 4.],\n",
      "          [4., 4., 4.],\n",
      "          [4., 4., 4.]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "z1 = conv1(image1)\n",
    "print(\"z1:\", z1)\n",
    "print(\"shape:\", z1.shape[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[[[1., 1.],\n",
       "                        [1., 1.]]]])),\n",
       "             ('bias', tensor([0.]))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv3 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2, stride = 2)\n",
    "conv3.state_dict()['weight'][0][0] = torch.tensor([[1.0, 1.0], [1.0, 1.0]])\n",
    "conv3.state_dict()['bias'][0] = 0.0\n",
    "conv3.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z3: tensor([[[[4., 4.],\n",
      "          [4., 4.]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "shape: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "z3 = conv3(image1)\n",
    "print(\"z3:\", z3)\n",
    "print(\"shape:\", z3.shape[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zero padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z4: tensor([[[[4.]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "z4: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "conv4 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2, stride=3)\n",
    "conv4.state_dict()['weight'][0][0] = torch.tensor([[1.0, 1.0], [1.0, 1.0]])\n",
    "conv4.state_dict()['bias'][0] = 0.0\n",
    "conv4.state_dict()\n",
    "z4 = conv4(image1)\n",
    "print(\"z4:\", z4)\n",
    "print(\"z4:\", z4.shape[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z5: tensor([[[[1., 2.],\n",
      "          [2., 4.]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "z5: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "conv5 = nn.Conv2d(in_channels = 1, out_channels = 1, kernel_size = 2, stride = 3, padding = 1)\n",
    "conv5.state_dict()['weight'][0][0] = torch.tensor([[1.0, 1.0], [1.0, 1.0]])\n",
    "conv5.state_dict()['bias'][0] = 0.0\n",
    "conv5.state_dict()\n",
    "z5 = conv5(image1)\n",
    "print(\"z5:\", z5)\n",
    "print(\"z5:\", z5.shape[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3338, -0.0270, -0.7508,  0.2999],\n",
       "          [ 1.0240,  0.7872,  1.1914, -0.7375],\n",
       "          [-0.2051, -0.5544,  0.5447, -0.7757],\n",
       "          [-0.3556, -0.8900, -0.9642, -0.8110]]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image = torch.randn((1, 1, 4, 4))\n",
    "Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z6: tensor([[[[0., 0.],\n",
      "          [0., 0.]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "z6: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "conv6 = nn.Conv2d(in_channels = 1, out_channels = 1, kernel_size=3)\n",
    "conv6.state_dict()['weight'][0][0] = torch.tensor([[0,0,0], [0,0,0], [0,0,0]])\n",
    "conv6.state_dict()['bias'][0] = 0.0\n",
    "z6 = conv6(Image)\n",
    "print(\"z6:\", z6)\n",
    "print(\"z6:\", z6.shape[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z7: tensor([[[[0., 0.],\n",
      "          [0., 0.]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "z7: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "conv7 = nn.Conv2d(in_channels = 1, out_channels = 1, kernel_size=2, stride=2)\n",
    "conv7.state_dict()['weight'][0][0] = torch.tensor([[0,0], [0,0]])\n",
    "conv7.state_dict()['bias'][0] = 0.0\n",
    "z7 = conv7(Image)\n",
    "print(\"z7:\", z7)\n",
    "print(\"z7:\", z7.shape[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
